# -*- coding: utf-8 -*-
"""Analisis Sentimen IKN 3AM

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/17fcrJA0Baq_ECQkgPJ6t4RDqFQJxGRL0

# Analisis Sentimen "IKN" by 3AM
"""

import pandas as pd

# Read the CSV file into a pandas DataFrame
df = pd.read_csv('/content/IKN 2500 data.csv', delimiter=",")
dfc = df.copy() #we keep a copy in case we need the original data set
df.head()

# Display the DataFrame
display(df)

# Cek jumlah data yang didapatkan
num_tweets = len(df)
print(f"Jumlah tweet dalam dataframe adalah {num_tweets}.")

df.drop_duplicates()

import pandas as pd
import re

# Anggap hasil scraping ada di df, kolom teks cuitan misalnya bernama 'full_text'
# df = pd.read_csv("hasil_scraping.csv")

# Kamus singkatan dan slang yang lebih lengkap
slang_dict = {
    'ikn': 'ibu kota nusantara', 'jkw': 'jokowi', 'owi': 'jokowi', 'prabowo': 'prabowo', 'owo': 'prabowo',
    'gibran': 'gibran', 'pdip': 'partai demokrasi indonesia perjuangan', 'sby': 'susilo bambang yudhoyono',
    'pd': 'partai demokrat', 'mbg': 'makan bergizi gratis', 'bansos': 'bantuan sosial', 'kaltim': 'kalimantan timur',
    'fufufafa': 'prabowo gibran', 'yg': 'yang', 'dgn': 'dengan', 'utk': 'untuk', 'sbg': 'sebagai', 'hrs': 'harus',
    'sdh': 'sudah', 'tdk': 'tidak', 'ga': 'tidak', 'gak': 'tidak', 'bgt': 'banget', 'bgtu': 'begitu', 'aja': 'saja',
    'smg': 'semoga', 'jg': 'juga', 'kpn': 'kapan', 'krn': 'karena', 'dlm': 'dalam', 'blm': 'belum', 'skrg': 'sekarang',
    'kmrn': 'kemarin', 'hrsnya': 'harusnya', 'bbrp': 'beberapa', 'dmn': 'di mana', 'gmn': 'bagaimana', 'gpp': 'tidak apa apa',
    'lol': 'tertawa terbahak-bahak', 'wkwk': 'tertawa', 'kzl': 'kesal', 'btw': 'ngomong-ngomong', 'plg': 'paling',
    'lg': 'lagi', 'trus': 'terus', 'pdhl': 'padahal', 'sbenernya': 'sebenarnya', 'kl': 'kalau', 'jd': 'jadi', 'dr': 'dari',
    'utuk': 'untuk', 'sm': 'sama', 'tp': 'tapi', 'moga': 'semoga', 'tks': 'terima kasih', 'thx': 'terima kasih',
    'gws': 'cepat sembuh', 'bgs': 'bagus', 'byk': 'banyak', 'sdkt': 'sedikit', 'mahal': 'mahal', 'murah': 'murah',
    'cpt': 'cepat', 'lambat': 'lambat', 'dkt': 'dekat', 'jauh': 'jauh', 'bnyk': 'banyak', 'dikit': 'sedikit',
    'jkt': 'jakarta', 'ri': 'republik indonesia', 'dki': 'daerah khusus ibukota'
}

# 1. Fungsi cleaning
def clean_text(text):
    text = text.lower()
    text = re.sub(r"http\S+|www\S+", "", text)
    text = re.sub(r"@\w+", "", text)
    text = re.sub(r"#", "", text)
    text = re.sub(r"[^a-zA-Z\s]", " ", text)
    text = re.sub(r"\s+", " ", text).strip()

    # Tambahkan langkah normalisasi singkatan
    words = text.split()
    normalized_words = [slang_dict.get(word, word) for word in words]
    return ' '.join(normalized_words)

# Terapkan ke semua teks
df["cleaned_content"] = df["full_text"].apply(clean_text)

# Cek hasilnya
print(df[["full_text", "cleaned_content"]].head(10))

import pandas as pd
import re
from nltk.tokenize import word_tokenize

# Pastikan Anda sudah mengunduh NLTK tokenizer
import nltk
nltk.download('punkt')
nltk.download('punkt_tab')

# Anggap df sudah memiliki kolom 'cleaned_content' hasil dari pre-processing sebelumnya
df = df.drop_duplicates(subset=["full_text"])
df = df.dropna(subset=["full_text"])

# ... (fungsi dan kamus pre-processing Anda ada di sini) ...

# Terapkan pre-processing untuk mendapatkan 'cleaned_content'
df["cleaned_content"] = df["full_text"].apply(clean_text)

# 1. Fungsi Tokenisasi
def tokenize_text(text):
    tokens = word_tokenize(text)
    return tokens

# 2. Terapkan tokenisasi ke kolom yang sudah bersih
df['tokens'] = df['cleaned_content'].apply(tokenize_text)

# Cek hasil tokenisasi
print(df[['full_text', 'cleaned_content', 'tokens']].head())

import pandas as pd
import re
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords

# Pastikan Anda sudah mengunduh NLTK stopwords
import nltk
nltk.download('stopwords')

# ... (Kode Pre-processing dan Tokenisasi Anda di sini) ...

# Ambil daftar stopwords bahasa Indonesia
list_stopwords = stopwords.words('indonesian')

# Tambahkan stopwords tambahan jika perlu
# Misalnya, kata 'ikn' mungkin ingin Anda pertahankan, tapi jika tidak, bisa ditambahkan di sini.
# Anda bisa tambahkan kata-kata umum yang tidak relevan dengan sentimen.
list_stopwords.extend(['iya','ya', 'tidak', 'tapi', 'saja', 'yah', 'kok', 'sih', 'dong'])

# Ubah list stopwords menjadi set untuk pencarian yang lebih cepat
set_stopwords = set(list_stopwords)

# 1. Fungsi Penghapus Stop Words
def remove_stopwords(tokens):
    # Filter token yang tidak ada dalam set stopwords
    filtered_tokens = [word for word in tokens if word not in set_stopwords]
    return filtered_tokens

# 2. Terapkan fungsi ke kolom 'tokens'
df['tokens_filtered'] = df['tokens'].apply(remove_stopwords)

# Cek hasil
print(df[['full_text', 'tokens', 'tokens_filtered']].head())

!pip install Sastrawi

import pandas as pd
from Sastrawi.Stemmer.StemmerFactory import StemmerFactory

# Pastikan Anda sudah memiliki dataframe 'df' dengan kolom 'tokens_filtered'
# ... (Kode Pre-processing, Tokenisasi, dan Stopwords di sini) ...

# 1. Inisialisasi Stemmer
factory = StemmerFactory()
stemmer = factory.create_stemmer()

# 2. Fungsi Stemming
def stem_tokens(tokens):
    # Gabungkan token menjadi kalimat lagi, karena stemmer Sastrawi bekerja lebih baik pada string
    text = " ".join(tokens)
    # Lakukan stemming
    stemmed_text = stemmer.stem(text)
    # Pisahkan lagi menjadi token
    stemmed_tokens = stemmed_text.split()
    return stemmed_tokens

# 3. Terapkan fungsi stemming ke kolom 'tokens_filtered'
df['stemmed_tokens'] = df['tokens_filtered'].apply(stem_tokens)

# Cek hasilnya
print(df[['full_text', 'tokens', 'tokens_filtered', 'stemmed_tokens']].head())

# Install transformers dan torch
!pip install transformers torch

from transformers import pipeline
from transformers import AutoTokenizer, AutoModelForSequenceClassification

# Load model IndoBERT Sentiment
pretrained = "mdhugol/indonesia-bert-sentiment-classification"

model = AutoModelForSequenceClassification.from_pretrained(pretrained)
tokenizer = AutoTokenizer.from_pretrained(pretrained)

# Buat pipeline untuk sentiment analysis
sentiment_analysis = pipeline("sentiment-analysis", model=model, tokenizer=tokenizer)

# Mapping label
label_index = {'LABEL_0': 'positive', 'LABEL_1': 'neutral', 'LABEL_2': 'negative'}

# Terapkan ke dataframe
def get_sentiment(text):
    try:
        result = sentiment_analysis(text[:512])  # BERT limit 512 token
        label = label_index[result[0]['label']]
        score = result[0]['score']
        return pd.Series([label, score])
    except:
        return pd.Series([None, None])

df[['sentiment', 'sentiment_score']] = df['cleaned_content'].apply(get_sentiment)

# Lihat hasil
print(df[['full_text','cleaned_content','sentiment','sentiment_score']].head(10))

from google.colab import drive
drive.mount('/content/drive')

# Simpan hasil analisis ke Google Drive
df.to_csv("/content/drive/MyDrive/ikn_sentiment_result.csv", index=False)

from google.colab import files

# Simpan sementara di Colab
df.to_csv("ikn_sentiment_result.csv", index=False)

# Download ke laptop/PC
files.download("ikn_sentiment_result.csv")